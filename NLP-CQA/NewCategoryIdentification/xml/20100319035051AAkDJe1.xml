<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<ResultSet xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="urn:yahoo:answers" xsi:schemaLocation="urn:yahoo:answers http://answers.yahooapis.com/AnswersService/V1/AnswerResponse.xsd">
  <Question id="20100319035051AAkDJe1" type="Answered">
    <Subject>how do i add urls to my search engine?</Subject>
    <Content>i made my own search engine with php.the problem is,i don&#039;t know how to add web pages to it(urls)if anyone knows,please help
</Content>
    <Date>2010-03-19 03:50:51</Date>
    <Timestamp>1268995851</Timestamp>
    <Link>http://answers.yahoo.com/question/?qid=20100319035051AAkDJe1</Link>
    <Category id="2115500139">Other - Internet</Category>
    <UserId>v3oq8G3Maa</UserId>
    <UserNick>None</UserNick>
    <UserPhotoURL>http://l.yimg.com/q/users/1Fw-iBcNSAAEE_GGRvKYu64c9OA==.medium.jpg</UserPhotoURL>
    <NumAnswers>1</NumAnswers>
    <NumComments>0</NumComments>
    <ChosenAnswer>Not sure how exactly your search engine works, but your search engine should have a program that acts as a &#039;spider&#039;. 

A spider &#039;crawls&#039; (i.e. downloads) a web page, indexes it, and adds any links it finds to a queue. When that web page has been indexed, the crawler takes the next url from the queue and indexes that one, again adding any links it finds to the queue. The spider continues indexing sites in this way indefinitely (web pages are constantly changing and need to be re-indexed, new sites are created everyday - crawling is a neverending process).

The major search engines also allow website owners to manually add their website to the queue to be crawled.

There are also files on many websites which help crawlers know what pages to index. The robots.txt file gives crawlers instructions on which folders it is allowed to index - your crawler should be programmed to respect the website owners requests in the robots.txt file. The robots.txt file can also give the crawler instructions on where to find a sitemap for the website. A sitemap is a file that contains XML code which lists all the pages on a website, which helps crawlers find all the site&#039;s pages and index them.

I&#039;m actually a little surprised that you have made a search engine, but don&#039;t seem to understand what a web crawler does. 

Starting a search engine is actually a really big task, and will take a lot of resources and money. A crawler would need to run constantly to index as many pages as possible. your crawler would also need to be able to figure out when to go back and re-index a web page, as they are constantly updated. Search engines like Google have many, many servers running programs to crawl the internet, and they still can&#039;t index everything quick enough.

If you do have some kind of crawler, I&#039;d say a good place to start indexing would be dmoz.org, which is one of the oldest directories of sites on the internet. Dmoz would provide a huge number of urls to initially index, as well as thousands of urls to sites, which would all provide urls to more sites, which would provide you with more urls, etc etc. Other good places to start to crawl would be social bookmarking sites like digg, stumbleupon and technorati.

Goodluck!</ChosenAnswer>
    <ChosenAnswererId>8963ed45af3c4bbe909ce1869e2957ddaa</ChosenAnswererId>
    <ChosenAnswererNick>Richard D</ChosenAnswererNick>
    <ChosenAnswerTimestamp>1268998705</ChosenAnswerTimestamp>
    <ChosenAnswerAwardTimestamp>1269175899</ChosenAnswerAwardTimestamp>
    <Answers>
        <Answer>
            <Content>Not sure how exactly your search engine works, but your search engine should have a program that acts as a &#039;spider&#039;. 

A spider &#039;crawls&#039; (i.e. downloads) a web page, indexes it, and adds any links it finds to a queue. When that web page has been indexed, the crawler takes the next url from the queue and indexes that one, again adding any links it finds to the queue. The spider continues indexing sites in this way indefinitely (web pages are constantly changing and need to be re-indexed, new sites are created everyday - crawling is a neverending process).

The major search engines also allow website owners to manually add their website to the queue to be crawled.

There are also files on many websites which help crawlers know what pages to index. The robots.txt file gives crawlers instructions on which folders it is allowed to index - your crawler should be programmed to respect the website owners requests in the robots.txt file. The robots.txt file can also give the crawler instructions on where to find a sitemap for the website. A sitemap is a file that contains XML code which lists all the pages on a website, which helps crawlers find all the site&#039;s pages and index them.

I&#039;m actually a little surprised that you have made a search engine, but don&#039;t seem to understand what a web crawler does. 

Starting a search engine is actually a really big task, and will take a lot of resources and money. A crawler would need to run constantly to index as many pages as possible. your crawler would also need to be able to figure out when to go back and re-index a web page, as they are constantly updated. Search engines like Google have many, many servers running programs to crawl the internet, and they still can&#039;t index everything quick enough.

If you do have some kind of crawler, I&#039;d say a good place to start indexing would be dmoz.org, which is one of the oldest directories of sites on the internet. Dmoz would provide a huge number of urls to initially index, as well as thousands of urls to sites, which would all provide urls to more sites, which would provide you with more urls, etc etc. Other good places to start to crawl would be social bookmarking sites like digg, stumbleupon and technorati.

Goodluck!</Content>
            <Reference></Reference>
            <Best>5</Best>
            <UserId>8963ed45af3c4bbe909ce1869e2957ddaa</UserId>
            <UserNick>Richard D</UserNick>
            <Date>2010-03-19 04:38:25</Date>
            <Timestamp>1268998705</Timestamp>
        </Answer>
    </Answers>
    <Comments>
    </Comments>
  </Question>
</ResultSet>
<!-- ws14.ydn.gq1.yahoo.com uncompressed/chunked Tue Apr  6 20:36:44 PDT 2010 -->
